{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051b577e-9862-4962-ac68-0407162f2a89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Estimate Intrinsic Dimension of Wikipedia Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b5467-4173-418e-a8b5-e1fa9d2bdc97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Cohere Wikipedia embeddings from Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=\"YourOwnHuggingFaceToken\")\n",
    "\n",
    "print(\"Loading the complete Cohere Wikipedia embeddings dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the entire dataset (non-streaming mode)\n",
    "# This will load everything into memory at once\n",
    "docs = load_dataset(\"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\")\n",
    "\n",
    "loading_time = time.time() - start_time\n",
    "print(f\"Dataset loaded in {loading_time:.2f} seconds\")\n",
    "# print(f\"Dataset size: {len(docs['train'])} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfc732-cb9c-43eb-ab48-360fbe741a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding vectors from dataset to numpy array\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Get the total number of rows\n",
    "num_rows = len(docs)\n",
    "print(f\"Total number of rows: {num_rows}\")\n",
    "\n",
    "# Define chunk size for processing and saving\n",
    "# Each embedding is 768 * 4 bytes (float32) ≈ 3 KB\n",
    "# Processing 1M embeddings at a time ≈ 3 GB of memory\n",
    "chunk_size = 1000000\n",
    "num_chunks = (num_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "output_dir = \"D:/WikipediaEmbeddings_Cohere/wikipedia_embeddings_chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Processing and saving embeddings in chunks...\")\n",
    "extract_start = time.time()\n",
    "\n",
    "for chunk_idx in range(num_chunks):\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * chunk_size, num_rows)\n",
    "    current_size = end_idx - start_idx\n",
    "    \n",
    "    print(f\"Processing chunk {chunk_idx+1}/{num_chunks} (rows {start_idx} to {end_idx-1})...\")\n",
    "    \n",
    "    # Process this chunk in smaller batches\n",
    "    batch_size = 10000\n",
    "    num_batches = (current_size + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Initialize array just for this chunk\n",
    "    chunk_embeddings = np.zeros((current_size, 768), dtype=np.float32)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = start_idx + batch_idx * batch_size\n",
    "        batch_end = min(start_idx + (batch_idx + 1) * batch_size, end_idx)\n",
    "        \n",
    "        # FIXED: Properly access Hugging Face dataset\n",
    "        # Get batch of examples using the proper slicing for datasets.arrow_dataset.Dataset\n",
    "        batch = docs.select(range(batch_start, batch_end))\n",
    "        \n",
    "        # FIXED: Extract embeddings using the proper method for accessing features\n",
    "        # Assuming 'emb' is a feature in your dataset\n",
    "        batch_embeddings = np.array(batch['emb'], dtype=np.float32)\n",
    "        \n",
    "        # Store in the chunk array\n",
    "        local_start = batch_start - start_idx\n",
    "        local_end = batch_end - start_idx\n",
    "        chunk_embeddings[local_start:local_end] = batch_embeddings\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == num_batches:\n",
    "            print(f\"  Processed batch {batch_idx+1}/{num_batches}\")\n",
    "    \n",
    "    # Save this chunk to disk\n",
    "    chunk_filename = os.path.join(output_dir, f\"embeddings_chunk_{chunk_idx:03d}.npy\")\n",
    "    np.save(chunk_filename, chunk_embeddings)\n",
    "    print(f\"Saved chunk {chunk_idx+1} to {chunk_filename}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del chunk_embeddings\n",
    "    \n",
    "print(f\"All processing completed in {time.time() - extract_start:.2f} seconds\")\n",
    "\n",
    "# Create a metadata file with information about the chunks\n",
    "with open(os.path.join(output_dir, \"metadata.txt\"), \"w\") as f:\n",
    "    f.write(f\"Total embeddings: {num_rows}\\n\")\n",
    "    f.write(f\"Embedding dimensions: 768\\n\")\n",
    "    f.write(f\"Number of chunks: {num_chunks}\\n\")\n",
    "    f.write(f\"Chunk size: {chunk_size}\\n\")\n",
    "    f.write(\"Data type: float32\\n\")\n",
    "    f.write(\"File format: NumPy .npy\\n\")\n",
    "\n",
    "# OPTIONAL: To recover disk space on SSD C:, manually delete downloaded dataset here after execution of this cell: C:\\Users\\{username}\\.cache\\huggingface\\datasets\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0a9df-a22d-4e13-bdfc-5e08f8d98a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring embeddings into SSD C: from slower USB drive D:\n",
    "\n",
    "import shutil\n",
    "\n",
    "# This won't fail if destination exists (Python 3.8+)\n",
    "shutil.copytree(\"D:/WikipediaEmbeddings_Cohere/wikipedia_embeddings_chunks\", \"C:/Users/rozmu/Documents/JupyterLabStuff/IntrinsicDimWikipedia/wikipedia_embeddings_chunks\", dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaeede-79c7-45bd-82b1-11a50b57a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get randomly sampled X% of embedding vectors\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "X = 0.1\n",
    "\n",
    "def reservoir_sample_vectors(directory, sample_percentage, seed=None):\n",
    "    \"\"\"\n",
    "    Memory-efficient reservoir sampling of vectors from .npy files.\n",
    "    This approach maintains constant memory usage regardless of total data size.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    npy_files = list(Path(directory).glob(\"*.npy\"))\n",
    "    if not npy_files:\n",
    "        raise ValueError(f\"No .npy files found in {directory}\")\n",
    "    \n",
    "    # First pass: count total vectors\n",
    "    total_vectors = 0\n",
    "    for file_path in npy_files:\n",
    "        arr = np.load(file_path, mmap_mode='r')\n",
    "        total_vectors += arr.shape[0]\n",
    "    \n",
    "    sample_count = int(total_vectors * sample_percentage / 100)\n",
    "    print(f\"Reservoir sampling {sample_count:,} vectors from {total_vectors:,} total\")\n",
    "    \n",
    "    # Initialize reservoir\n",
    "    reservoir = None\n",
    "    vectors_seen = 0\n",
    "    \n",
    "    for file_path in npy_files:\n",
    "        arr = np.load(file_path, mmap_mode='r')\n",
    "        \n",
    "        for i in range(arr.shape[0]):\n",
    "            vectors_seen += 1\n",
    "            \n",
    "            if reservoir is None:\n",
    "                # Initialize reservoir with first vectors\n",
    "                reservoir = np.zeros((sample_count, 768), dtype=arr.dtype)\n",
    "            \n",
    "            if len(reservoir) < sample_count:\n",
    "                # Fill reservoir\n",
    "                reservoir[len(reservoir)] = arr[i]\n",
    "            else:\n",
    "                # Replace with probability sample_count/vectors_seen\n",
    "                j = random.randint(0, vectors_seen - 1)\n",
    "                if j < sample_count:\n",
    "                    reservoir[j] = arr[i]\n",
    "        \n",
    "        print(f\"Processed {file_path.name}: {vectors_seen:,} vectors seen\")\n",
    "    \n",
    "    return reservoir[:min(sample_count, vectors_seen)]\n",
    "\n",
    "sampled_data = reservoir_sample_vectors(\"wikipedia_embeddings_chunks\", X, seed=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c996b-c75b-4843-8f11-d66142750651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate intrinsic dimension of sampled vectors by correlation dimension method.\n",
    "\n",
    "import skdim\n",
    "import os\n",
    "\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '1'\n",
    "\n",
    "Sample_Count = sampled_data.shape[0]\n",
    "for i in range(4):\n",
    "    proportion_smallest = 0.5-(i+1)*0.1\n",
    "    proportion_largest = 0.5+(i+1)*0.1\n",
    "    ID_Estimator = skdim.id.CorrInt(int(proportion_smallest*Sample_Count),int(proportion_largest*Sample_Count))\n",
    "    IntrinsicDim = ID_Estimator.fit_transform(sampled_data)\n",
    "    print(proportion_smallest,proportion_largest,IntrinsicDim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intrinsic_dim]",
   "language": "python",
   "name": "conda-env-intrinsic_dim-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
